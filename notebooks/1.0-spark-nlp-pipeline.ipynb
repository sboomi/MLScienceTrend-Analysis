{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aec6012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/User/Documents/Projects/Python-projects/Work/MLScienceTrend-Analysis/data/raw/arxiv_manifest/arxiv-metadata-oai-snapshot.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path().resolve().parent\n",
    "DATA_DIR = ROOT_DIR / 'data' / 'raw'\n",
    "\n",
    "json_file = DATA_DIR / 'arxiv_manifest' / 'arxiv-metadata-oai-snapshot.json'\n",
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0039b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 3.3.0\n",
      "Apache Spark version: 3.1.1\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b35173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- authors_parsed: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- journal-ref: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- report-no: string (nullable = true)\n",
      " |-- submitter: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- update_date: string (nullable = true)\n",
      " |-- versions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- created: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arxiv_df = spark.read.json(json_file.resolve().as_posix())\n",
    "arxiv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bab7f26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----------------+\n",
      "|                                          abstract|       categories|\n",
      "+--------------------------------------------------+-----------------+\n",
      "|  A fully differential calculation in perturbat...|           hep-ph|\n",
      "|  We describe a new algorithm, the $(k,\\ell)$-p...|    math.CO cs.CG|\n",
      "|  The evolution of Earth-Moon system is describ...|   physics.gen-ph|\n",
      "|  We show that a determinant of Stirling cycle ...|          math.CO|\n",
      "|  In this paper we show how to compute the $\\La...|  math.CA math.FA|\n",
      "|  We study the two-particle wave function of pa...|cond-mat.mes-hall|\n",
      "|  A rather non-standard quantum representation ...|            gr-qc|\n",
      "|  A general formulation was developed to repres...|cond-mat.mtrl-sci|\n",
      "|  We discuss the results from the combined IRAC...|         astro-ph|\n",
      "|  Partial cubes are isometric subgraphs of hype...|          math.CO|\n",
      "|  In this paper we present an algorithm for com...|  math.NT math.AG|\n",
      "|  Recently, Bruinier and Ono classified cusp fo...|          math.NT|\n",
      "|  Serre obtained the p-adic limit of the integr...|          math.NT|\n",
      "|  In this article we discuss a relation between...|  math.CA math.AT|\n",
      "|  The pure spinor formulation of the ten-dimens...|           hep-th|\n",
      "|  In this work, we evaluate the lifetimes of th...|           hep-ph|\n",
      "|  Results from spectroscopic observations of th...|         astro-ph|\n",
      "|  We give a prescription for how to compute the...|           hep-th|\n",
      "|  In this note we give a new method for getting...|  math.PR math.AG|\n",
      "|  The shape of the hadronic form factor f+(q2) ...|           hep-ex|\n",
      "+--------------------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['abstract', 'categories']\n",
    "\n",
    "arxiv_df.select(columns).show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35034896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----------------+-------+\n",
      "|                                          abstract|       categories|cat_num|\n",
      "+--------------------------------------------------+-----------------+-------+\n",
      "|  A fully differential calculation in perturbat...|           hep-ph|      1|\n",
      "|  We describe a new algorithm, the $(k,\\ell)$-p...|    math.CO cs.CG|      2|\n",
      "|  The evolution of Earth-Moon system is describ...|   physics.gen-ph|      1|\n",
      "|  We show that a determinant of Stirling cycle ...|          math.CO|      1|\n",
      "|  In this paper we show how to compute the $\\La...|  math.CA math.FA|      2|\n",
      "|  We study the two-particle wave function of pa...|cond-mat.mes-hall|      1|\n",
      "|  A rather non-standard quantum representation ...|            gr-qc|      1|\n",
      "|  A general formulation was developed to repres...|cond-mat.mtrl-sci|      1|\n",
      "|  We discuss the results from the combined IRAC...|         astro-ph|      1|\n",
      "|  Partial cubes are isometric subgraphs of hype...|          math.CO|      1|\n",
      "|  In this paper we present an algorithm for com...|  math.NT math.AG|      2|\n",
      "|  Recently, Bruinier and Ono classified cusp fo...|          math.NT|      1|\n",
      "|  Serre obtained the p-adic limit of the integr...|          math.NT|      1|\n",
      "|  In this article we discuss a relation between...|  math.CA math.AT|      2|\n",
      "|  The pure spinor formulation of the ten-dimens...|           hep-th|      1|\n",
      "|  In this work, we evaluate the lifetimes of th...|           hep-ph|      1|\n",
      "|  Results from spectroscopic observations of th...|         astro-ph|      1|\n",
      "|  We give a prescription for how to compute the...|           hep-th|      1|\n",
      "|  In this note we give a new method for getting...|  math.PR math.AG|      2|\n",
      "|  The shape of the hadronic form factor f+(q2) ...|           hep-ex|      1|\n",
      "+--------------------------------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, size, split\n",
    "\n",
    "arxiv_df.withColumn('cat_num', size(split(col(\"categories\"), r\" \"))).select(columns + ['cat_num']).show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39847822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='cat_num'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "cats_num = arxiv_df.withColumn('cat_num', size(split(col(\"categories\"), r\" \")))\\\n",
    "    .groupBy(\"cat_num\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.col(\"count\").desc()) \\\n",
    "    .toPandas()\n",
    "\n",
    "\n",
    "cats_num.plot.bar(x='cat_num', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bd04c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physics\n",
      "astro-ph\n",
      "astro-ph\n",
      "astro-ph.GA\n",
      "astro-ph.CO\n",
      "astro-ph.EP\n",
      "astro-ph.HE\n",
      "astro-ph.IM\n",
      "astro-ph.SR\n",
      "cond-mat\n",
      "cond-mat\n",
      "cond-mat.dis-nn\n",
      "cond-mat.mtrl-sci\n",
      "cond-mat.mes-hall\n",
      "cond-mat.other\n",
      "cond-mat.quant-gas\n",
      "cond-mat.soft\n",
      "cond-mat.stat-mech\n",
      "cond-mat.str-el\n",
      "cond-mat.supr-con\n",
      "gr-qc\n",
      "gr-qc\n",
      "hep-ex\n",
      "hep-ex\n",
      "hep-lat\n",
      "hep-lat\n",
      "hep-ph\n",
      "hep-ph\n",
      "hep-th\n",
      "hep-th\n",
      "math-ph\n",
      "math-ph\n",
      "nlin\n",
      "nlin\n",
      "nlin.AO\n",
      "nlin.CG\n",
      "nlin.CD\n",
      "nlin.SI\n",
      "nlin.PS\n",
      "nucl-ex\n",
      "nucl-ex\n",
      "nucl-th\n",
      "nucl-th\n",
      "physics\n",
      "physics\n",
      "physics.acc-ph\n",
      "physics.app-ph\n",
      "physics.ao-ph\n",
      "physics.atm-clus\n",
      "physics.atom-ph\n",
      "physics.bio-ph\n",
      "physics.chem-ph\n",
      "physics.class-ph\n",
      "physics.comp-ph\n",
      "physics.data-an\n",
      "physics.flu-dyn\n",
      "physics.gen-ph\n",
      "physics.geo-ph\n",
      "physics.hist-ph\n",
      "physics.ins-det\n",
      "physics.med-ph\n",
      "physics.optics\n",
      "physics.soc-ph\n",
      "physics.ed-ph\n",
      "physics.plasm-ph\n",
      "physics.pop-ph\n",
      "physics.space-ph\n",
      "quant-ph\n",
      "quant-ph\n",
      "=================\n",
      "Mathematics\n",
      "math\n",
      "math\n",
      "math.AG\n",
      "math.AT\n",
      "math.AP\n",
      "math.CT\n",
      "math.CA\n",
      "math.CO\n",
      "math.AC\n",
      "math.CV\n",
      "math.DG\n",
      "math.DS\n",
      "math.FA\n",
      "math.GM\n",
      "math.GN\n",
      "math.GT\n",
      "math.GR\n",
      "math.HO\n",
      "math.IT\n",
      "math.KT\n",
      "math.LO\n",
      "math.MP\n",
      "math.MG\n",
      "math.NT\n",
      "math.NA\n",
      "math.OA\n",
      "math.OC\n",
      "math.PR\n",
      "math.QA\n",
      "math.RT\n",
      "math.RA\n",
      "math.SP\n",
      "math.ST\n",
      "math.SG\n",
      "=================\n",
      "Computer Science\n",
      "cs\n",
      "cs\n",
      "cs.AI\n",
      "cs.CL\n",
      "cs.CC\n",
      "cs.CE\n",
      "cs.CG\n",
      "cs.GT\n",
      "cs.CV\n",
      "cs.CY\n",
      "cs.CR\n",
      "cs.DS\n",
      "cs.DB\n",
      "cs.DL\n",
      "cs.DM\n",
      "cs.DC\n",
      "cs.ET\n",
      "cs.FL\n",
      "cs.GL\n",
      "cs.GR\n",
      "cs.AR\n",
      "cs.HC\n",
      "cs.IR\n",
      "cs.IT\n",
      "cs.LO\n",
      "cs.LG\n",
      "cs.MS\n",
      "cs.MA\n",
      "cs.MM\n",
      "cs.NI\n",
      "cs.NE\n",
      "cs.NA\n",
      "cs.OS\n",
      "cs.OH\n",
      "cs.PF\n",
      "cs.PL\n",
      "cs.RO\n",
      "cs.SI\n",
      "cs.SE\n",
      "cs.SD\n",
      "cs.SC\n",
      "cs.SY\n",
      "=================\n",
      "Quantitative Biology\n",
      "q-bio\n",
      "q-bio\n",
      "q-bio.BM\n",
      "q-bio.CB\n",
      "q-bio.GN\n",
      "q-bio.MN\n",
      "q-bio.NC\n",
      "q-bio.OT\n",
      "q-bio.PE\n",
      "q-bio.QM\n",
      "q-bio.SC\n",
      "q-bio.TO\n",
      "=================\n",
      "Quantitative Finance\n",
      "q-fin\n",
      "q-fin\n",
      "q-fin.CP\n",
      "q-fin.EC\n",
      "q-fin.GN\n",
      "q-fin.MF\n",
      "q-fin.PM\n",
      "q-fin.PR\n",
      "q-fin.RM\n",
      "q-fin.ST\n",
      "q-fin.TR\n",
      "=================\n",
      "Statistics\n",
      "stat\n",
      "stat\n",
      "stat.AP\n",
      "stat.CO\n",
      "stat.ML\n",
      "stat.ME\n",
      "stat.OT\n",
      "stat.TH\n",
      "=================\n",
      "Electrical Engineering and Systems Science\n",
      "eess\n",
      "eess\n",
      "eess.AS\n",
      "eess.IV\n",
      "eess.SP\n",
      "eess.SY\n",
      "=================\n",
      "Economics\n",
      "econ\n",
      "econ\n",
      "econ.EM\n",
      "econ.GN\n",
      "econ.TH\n",
      "=================\n",
      "About arXiv\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "cats = {}\n",
    "\n",
    "pat = re.compile(r\"\\/(list|archive)\\/(.+)\\/(new|recent)?\")\n",
    "\n",
    "r = requests.get(\"https://arxiv.org/\")\n",
    "if r.status_code == 200:\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    for title, bullet_pt in zip(soup.select(\"div#content h2\"), soup.select(\"div#content h2 ~ ul\")):\n",
    "        print(title.text)\n",
    "        cats[title.text] = []\n",
    "        for pt in bullet_pt.select('li'):\n",
    "            for href in [link['href'] for link in pt.find_all(\"a\")]:\n",
    "                match = pat.search(href)\n",
    "                if match:\n",
    "                    print(match.group(2))\n",
    "                    cats[title.text].append(match.group(2))\n",
    "        print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8d210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----------------+\n",
      "|                                          abstract|       categories|\n",
      "+--------------------------------------------------+-----------------+\n",
      "|  A fully differential calculation in perturbat...|           hep-ph|\n",
      "|  The evolution of Earth-Moon system is describ...|   physics.gen-ph|\n",
      "|  We show that a determinant of Stirling cycle ...|          math.CO|\n",
      "|  We study the two-particle wave function of pa...|cond-mat.mes-hall|\n",
      "|  A rather non-standard quantum representation ...|            gr-qc|\n",
      "|  A general formulation was developed to repres...|cond-mat.mtrl-sci|\n",
      "|  We discuss the results from the combined IRAC...|         astro-ph|\n",
      "|  Partial cubes are isometric subgraphs of hype...|          math.CO|\n",
      "|  Recently, Bruinier and Ono classified cusp fo...|          math.NT|\n",
      "|  Serre obtained the p-adic limit of the integr...|          math.NT|\n",
      "|  The pure spinor formulation of the ten-dimens...|           hep-th|\n",
      "|  In this work, we evaluate the lifetimes of th...|           hep-ph|\n",
      "|  Results from spectroscopic observations of th...|         astro-ph|\n",
      "|  We give a prescription for how to compute the...|           hep-th|\n",
      "|  The shape of the hadronic form factor f+(q2) ...|           hep-ex|\n",
      "|  We present Lie group integrators for nonlinea...|          math.NA|\n",
      "|  The very nature of the solar chromosphere, it...|         astro-ph|\n",
      "|  The formation of quasi-2D spin-wave waveforms...|          nlin.PS|\n",
      "|  Zero-divisors (ZDs) derived by Cayley-Dickson...|          math.RA|\n",
      "|  We describe a peculiar fine structure acquire...|cond-mat.mes-hall|\n",
      "+--------------------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do a simple model with unique categories\n",
    "\n",
    "arxiv_df.select(columns)\\\n",
    "    .filter(size(split(col(\"categories\"), r\" \")) == 1)\\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3960c469",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o143.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 463) (DESKTOP-0V4EL69 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12804/2921395303.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0marxiv_df_2\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0marxiv_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"categories\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'simple_cats'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_cat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mltrends\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mltrends\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mltrends\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mltrends\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 463) (DESKTOP-0V4EL69 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def find_alias(cat: str, cat_dict = cats) -> str:\n",
    "    for k, v in cats.items():\n",
    "        if v[0] in cat:\n",
    "            return k\n",
    "        \n",
    "    return \"No cat\"\n",
    "\n",
    "@pandas_udf('string')\n",
    "def replace_cat(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(lambda x: find_alias(x))\n",
    "\n",
    "\n",
    "\n",
    "arxiv_df_2 =arxiv_df.select(columns)\\\n",
    "    .filter(size(split(col(\"categories\"), r\" \")) == 1)\\\n",
    "    .withColumn('simple_cats', replace_cat())\\\n",
    "    .select(columns + ['simple_cats'])\\\n",
    "    .show(truncate=50)\n",
    "\n",
    "arxiv_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7f45eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Physics': ['hep-lat',\n",
       "  'cond-mat',\n",
       "  'nucl-ex',\n",
       "  'physics',\n",
       "  'hep-ex',\n",
       "  'nucl-th',\n",
       "  'hep-ph',\n",
       "  'math-ph',\n",
       "  'hep-th',\n",
       "  'nlin',\n",
       "  'astro-ph',\n",
       "  'quant-ph',\n",
       "  'gr-qc'],\n",
       " 'Mathematics': ['math'],\n",
       " 'Computer Science': ['cs'],\n",
       " 'Quantitative Biology': ['q-bio'],\n",
       " 'Quantitative Finance': ['q-fin'],\n",
       " 'Statistics': ['stat'],\n",
       " 'Electrical Engineering and Systems Science': ['eess'],\n",
       " 'Economics': ['econ']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_cats = {}\n",
    "\n",
    "for k, v in cats.items():\n",
    "    s_cats = set([val.split('.')[0] for val in v])\n",
    "    \n",
    "    if list(s_cats):\n",
    "        simple_cats[k] = list(s_cats)\n",
    "    \n",
    "simple_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "arxiv_df.withColumn('super_categories', upper(df.c)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
